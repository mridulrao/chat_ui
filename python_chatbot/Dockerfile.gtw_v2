# Dockerfile
ARG TARGETPLATFORM=linux/amd64
FROM --platform=$TARGETPLATFORM nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PIP_NO_CACHE_DIR=1


RUN apt-get update && apt-get install -y --no-install-recommends \
    python3 python3-pip python3-venv ca-certificates curl git \
 && rm -rf /var/lib/apt/lists/*


RUN pip3 install --upgrade pip && \
    pip3 install --index-url https://download.pytorch.org/whl/cu121 torch && \
    pip3 install \
      "vllm>=0.6.2" \
      fastapi uvicorn uvloop httpx orjson pydantic-settings \
      redis

# App layout
WORKDIR /app
COPY gateway_v3.py /app/gateway.py

# Models cache (persistent volume recommended)
ENV VLLM_DOWNLOAD_DIR=/models
RUN mkdir -p /models

# Default env (override in Runpod)
ENV MODEL_NAME="Qwen/Qwen3-4B-Instruct-2507" \
    OPENAI_HOST=0.0.0.0 \
    OPENAI_PORT=8000 \
    GATEWAY_HOST=0.0.0.0 \
    GATEWAY_PORT=3000 \
    API_KEYS="devkey" \
    MAX_TURNS=24 \
    SESSION_TTL_SECONDS=3600 \
    UPSTREAM_OPENAI="http://127.0.0.1:8000/v1" \
    REDIS_URL=""

# Expose ports
EXPOSE 8000 3000


CMD ["/bin/bash", "-lc", "\
  python3 -m vllm.entrypoints.openai.api_server \
    --model ${MODEL_NAME} \
    --host ${OPENAI_HOST} --port ${OPENAI_PORT} \
    --download-dir ${VLLM_DOWNLOAD_DIR} \
    --max-model-len 8192 \
    --gpu-memory-utilization 0.85 \
    --tensor-parallel-size 1 \
    --enforce-eager \
  & \
  uvicorn gateway:app --host ${GATEWAY_HOST} --port ${GATEWAY_PORT} \
"]
